{"cells":[{"cell_type":"markdown","source":["## DS-3002: Sample Capstone Project\nThis notebook demonstrates many of the software libraries and programming techniques required to fulfill the requirements of the final end-of-session capstone project for course **DS-3002: Data Systems** at the University of Virginia School of Data Science. The spirit of the project is to provide a capstone challenge that requires students to demonstrate a practical and functional understanding of each of the data systems and architectural principles covered throughout the session.\n\n**These include:**\n- Relational Database Management Systems (e.g., MySQL, Microsoft SQL Server, Oracle, IBM DB2)\n  - Online Transaction Processing Systems (OLTP): *Relational Databases Optimized for High-Volume Write Operations; Normalized to 3rd Normal Form.*\n  - Online Analytical Processing Systems (OLAP): *Relational Databases Optimized for Read/Aggregation Operations; Dimensional Model (i.e, Star Schema)*\n- NoSQL *(Not Only SQL)* Systems (e.g., MongoDB, CosmosDB, Cassandra, HBase, Redis)\n- File System *(Data Lake)* Source Systems (e.g., AWS S3, Microsoft Azure Data Lake Storage)\n  - Various Datafile Formats (e.g., JSON, CSV, Parquet, Text, Binary)\n- Massively Parallel Processing *(MPP)* Data Integration Systems (e.g., Apache Spark, Databricks)\n- Data Integration Patterns (e.g., Extract-Transform-Load, Extract-Load-Transform, Extract-Load-Transform-Load, Lambda & Kappa Architectures)\n\nWhat's more, this project requires students to make effective decisions regarding whether to implement a Cloud-hosted, on-premises hosted, or hybrid architecture.\n\n### Section I: Prerequisites\n\n#### 1.0. Import Required Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fee938fe-e903-405e-8cb7-6f4e22d764ac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os\nimport json\nimport pymongo\nimport pyspark.pandas as pd  # This uses Koalas that is included in PySpark version 3.2 or newer.\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StructType, StructField, StringType, TimestampType, BinaryType\nfrom pyspark.sql.types import ByteType, ShortType, IntegerType, LongType, FloatType, DecimalType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71d6bb4d-98fb-4e6b-913c-04b7f9c9de4b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n\u001B[0;32m<command-479912801394481>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpymongo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m  \u001B[0;31m# This uses Koalas that is included in PySpark version 3.2 or newer.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pymongo'","errorSummary":"<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'pymongo'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n\u001B[0;32m<command-479912801394481>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpymongo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m  \u001B[0;31m# This uses Koalas that is included in PySpark version 3.2 or newer.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pymongo'"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 2.0. Instantiate Global Variables"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68145b1e-5001-49c4-818f-6f37b1845600","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Azure SQL Server Connection Information #####################\n# Had trouble connecting to my own SQL server so decided to use the one provided\njdbc_hostname = \"ds3002-sql.database.windows.net\"\njdbc_port = 1433\nsrc_database = \"AdventureWorksLT\"\n\nconnection_properties = {\n  \"user\" : \"root\",\n  \"password\" : \"Suchottv20!\",\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\n# MongoDB Atlas Connection Information ########################\n# using the atlas connection from Lab 4 (which is Tupitza's)\natlas_cluster_name = \"sandbox\"\natlas_database_name = \"sample_airbnb\"\natlas_user_name = \"m001-student\"\natlas_password = \"m001-mongodb-basics\"\n\n# Data Files (JSON) Information ###############################\ndst_database = \"Chinook\"\n\nbase_dir = \"dbfs:/FileStore/ds3002-capstone\"\ndatabase_dir = f\"{base_dir}/{dst_database}\"\n\ndata_dir = f\"{base_dir}/source_data\"\nbatch_dir = f\"{data_dir}/batch\"\nstream_dir = f\"{data_dir}/stream\"\n\noutput_bronze = f\"{database_dir}/fact_sales_orders/bronze\"\noutput_silver = f\"{database_dir}/fact_sales_orders/silver\"\noutput_gold   = f\"{database_dir}/fact_sales_orders/gold\"\n\n# Delete the Streaming Files ################################## \ndbutils.fs.rm(f\"{database_dir}/fact_sales_orders\", True)\n\n# Delete the Database Files ###################################\ndbutils.fs.rm(database_dir, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1f0b257-f65e-40cb-b518-0bcc358aad3d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.0. Define Global Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84f966b1-cd11-4968-bfcc-aa6aa2e06ca0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# ######################################################################################################################\n# Use this Function to Fetch a DataFrame from the Azure SQL database server.\n# ######################################################################################################################\ndef get_sql_dataframe(host_name, port, db_name, conn_props, sql_query):\n    '''Create a JDBC URL to the Azure SQL Database'''\n    jdbcUrl = f\"jdbc:sqlserver://{host_name}:{port};database={db_name}\"\n    \n    '''Invoke the spark.read.jdbc() function to query the database, and fill a Pandas DataFrame.'''\n    dframe = spark.read.jdbc(url=jdbcUrl, table=sql_query, properties=conn_props)\n    \n    return dframe\n\n\n# ######################################################################################################################\n# Use this Function to Fetch a DataFrame from the MongoDB Atlas database server Using PyMongo.\n# ######################################################################################################################\ndef get_mongo_dataframe(user_id, pwd, cluster_name, db_name, collection, conditions, projection, sort):\n    '''Create a client connection to MongoDB'''\n    mongo_uri = f\"mongodb+srv://{user_id}:{pwd}@{cluster_name}.zibbf.mongodb.net/{db_name}?retryWrites=true&w=majority\"\n    \n    client = pymongo.MongoClient(mongo_uri)\n\n    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n    db = client[db_name]\n    if conditions and projection and sort:\n        dframe = pd.DataFrame(list(db[collection].find(conditions, projection).sort(sort)))\n    elif conditions and projection and not sort:\n        dframe = pd.DataFrame(list(db[collection].find(conditions, projection)))\n    else:\n        dframe = pd.DataFrame(list(db[collection].find()))\n\n    client.close()\n    \n    return dframe\n\n# ######################################################################################################################\n# Use this Function to Create New Collections by Uploading JSON file(s) to the MongoDB Atlas server.\n# ######################################################################################################################\ndef set_mongo_collection(user_id, pwd, cluster_name, db_name, src_file_path, json_files):\n    '''Create a client connection to MongoDB'''\n    mongo_uri = f\"mongodb+srv://{user_id}:{pwd}@{cluster_name}.zibbf.mongodb.net/{db_name}?retryWrites=true&w=majority\"\n    client = pymongo.MongoClient(mongo_uri)\n    db = client[db_name]\n    \n    '''Read in a JSON file, and Use It to Create a New Collection'''\n    for file in json_files:\n        db.drop_collection(file)\n        json_file = os.path.join(src_file_path, json_files[file])\n        with open(json_file, 'r') as openfile:\n            json_object = json.load(openfile)\n            file = db[file]\n            result = file.insert_many(json_object)\n\n    client.close()\n    \n    return result"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74502bef-ecef-4710-9935-bb83fe7ad0bd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Section II: Populate Dimensions by Ingesting Reference (Cold-path) Data \n#### 1.0. Fetch Reference Data From an Azure SQL Database\n##### 1.1. Create a New Databricks Metadata Database, and then Create a New Table that Sources its Data from a View in an Azure SQL database."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb6addad-7adc-4088-af74-59a9cb91a2e0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nDROP DATABASE IF EXISTS Chinook CASCADE;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed9dc74c-5e46-4c63-83af-e14fee0c2762","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE DATABASE IF NOT EXISTS Chinkook\nCOMMENT \"Capstone Project Database\"\nLOCATION \"dbfs:/FileStore/ds3002-capstone/Chinook\"\nWITH DBPROPERTIES (contains_pii = true, purpose = \"DS-3002 Capstone Project\");"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40171e89-b36f-4d55-9efd-cc351b973b07","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW view_product\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url \"jdbc:sqlserver://ds3002-sql.database.windows.net:1433;database=AdventureWorksLT\",\n  dbtable \"SalesLT.vDimProducts\",\n  user \"root\",\n  password \"Suchottv20!\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3fcc76e-332d-42ff-a3bb-313c40a2da36","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nUSE DATABASE Chinook;\n\nCREATE TABLE IF NOT EXISTS Chinook.dim_customer\nCOMMENT \"Customer Dimension Table\"\nLOCATION \"dbfs:/FileStore/ds3002-capstone/adventure_works/dim_customer\"\nAS SELECT * FROM view_product"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9589eeeb-5b46-44e6-b2b7-8182c5f90c71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM Chinook.dim_customer LIMIT 5"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eae60e66-3b23-4c7c-95e3-69edba28565d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED Chinook.dim_customer;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e1a9ade-3343-4ce5-a3c1-2e31cfb4c556","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 1.2. Create a New Table that Sources its Data from a Table in an Azure SQL database."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3749f18e-dcc0-4937-8844-6a214f4b9abe","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW view_date\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url \"jdbc:sqlserver://ds3002-sql.database.windows.net:1433;database=AdventureWorksLT\",\n  dbtable \"dbo.DimDate\",\n  user \"root\",\n  password \"Suchottv20!\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"914eb5cf-87d4-4fff-8c3a-c7a6bea93d51","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nUSE DATABASE Chinook;\n\nCREATE TABLE IF NOT EXISTS Chinook.dim_employee\nCOMMENT \"Employee Dimension Table\"\nLOCATION \"dbfs:/FileStore/ds3002-capstone/adventure_works/dim_employee\"\nAS SELECT * FROM view_date"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a07aa15-8b96-457a-8c73-1b514f9b76f3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM Chinook.dim_employee LIMIT 5"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9f39776-cae1-455e-93ce-281ae7ea5c49","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED Chinook.dim_employee;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a46ac1ed-8124-45b6-bb1e-62c6647cb9a0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n##### 2.1. View the Data Files on the Databricks File System"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9f0b329-f795-477b-b95b-734070c317c1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(dbutils.fs.ls(batch_dir))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef88ade7-db6d-48db-a154-ed396be811d5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 2.2. Create a New MongoDB Database, and Load JSON Data Into a New MongoDB Collection\n**NOTE:** The following cell **can** be run more than once because the **set_mongo_collection()** function **is** idempotent."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"151b71fd-229c-442e-9a33-41e30b727be1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["source_dir = '/dbfs/FileStore/ds3002-capstone/source_data/batch'\njson_files = {\"customers\" : 'AdventureWorksLT_DimCustomer.json'}\n\nset_mongo_collection(atlas_user_name, atlas_password, atlas_cluster_name, atlas_database_name, source_dir, json_files) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c757a431-9aba-4b93-b642-15f80a1360cd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 2.3. Fetch Data from the New MongoDB Collection"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e5931ba-3eb0-4762-be24-282b0f1117d0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\nimport com.mongodb.spark._\n\nval df_customer = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", \"adventure_works\").option(\"collection\", \"customers\").load()\ndisplay(df_customer)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a10441c9-79c8-404b-a206-a6e971735f02","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\ndf_customer.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9737925e-fb34-448a-8f93-13a052389f42","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 2.4. Use the Spark DataFrame to Create a New Table in the Databricks (Adventure Works) Metadata Database"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45724671-ac76-460e-9069-10266b68ce20","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\ndf_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"adventure_works.dim_customer\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5d970b0-7b22-4bc9-924e-e51b3ac3507c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED chinook.dim_track"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44df606d-a56e-458f-92fe-4643fdb9340a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 2.5. Query the New Table in the Databricks Metadata Database"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6265ecf2-6713-4a9c-aaf3-b48bf93b2a89","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM Chinook.dim_track LIMIT 5"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc353d83-f9b1-4587-91ef-fa8e72e0ddf9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.0. Fetch Data from a File System\n##### 3.1. Use PySpark to Read From a CSV File"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d0ea7d5-d5be-4d71-8b30-0d65cb205f4e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["address_csv = f\"{batch_dir}/AdventureWorksLT_DimAddress.csv\"\n\ndf_address = spark.read.format('csv').options(header='true', inferSchema='true').load(address_csv)\ndisplay(df_address)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f199b417-75b3-480d-aab7-3e6cdb025598","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_address.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7802c9b2-cf39-4fa8-873a-67caefd1a308","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_address.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"adventure_works.dim_address\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9c20cb0a-c38d-4bd6-bd7c-23bb00099558","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED Chinook.dim_album;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47ff01eb-1aec-4aaf-9d06-4e0d5280059c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM Chinook.dim_album LIMIT 5;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"730483e6-8239-448d-ad43-41a35925e578","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Verify Dimension Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77837cbf-5598-4b31-a85c-5f9a957c3a12","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nUSE Chinook;\nSHOW TABLES"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2598296d-98cb-42be-b955-3f46c1c2ec06","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Section III: Integrate Reference Data with Real-Time Data\n#### 6.0. Use AutoLoader to Process Streaming (Hot Path) Data \n##### 6.1. Bronze Table: Process 'Raw' JSON Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ec44d85-a8cb-4dea-8fb3-8896a6ffb573","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.readStream\n .format(\"cloudFiles\")\n .option(\"cloudFiles.format\", \"json\")\n .option(\"cloudFiles.schemaHints\", \"SalesOrderID INT\")\n .option(\"cloudFiles.schemaHints\", \"RevisionNumber TINYINT\")\n .option(\"cloudFiles.schemaHints\", \"OrderDate TIMESTAMP\")\n .option(\"cloudFiles.schemaHints\", \"DueDate TIMESTAMP\") \n .option(\"cloudFiles.schemaHints\", \"ShipDate TIMESTAMP\")\n .option(\"cloudFiles.schemaHints\", \"Status TINYINT\")\n .option(\"cloudFiles.schemaHints\", \"OnlineOrderFlag BINARY\")\n .option(\"cloudFiles.schemaHints\", \"SalesOrderNumber STRING\")\n .option(\"cloudFiles.schemaHints\", \"PurchaseOrderNumber STRING\") \n .option(\"cloudFiles.schemaHints\", \"AccountNumber STRING\")\n .option(\"cloudFiles.schemaHints\", \"CustomerID INT\")\n .option(\"cloudFiles.schemaHints\", \"ShipToAddressID INT\")\n .option(\"cloudFiles.schemaHints\", \"BillToAddressID INT\")\n .option(\"cloudFiles.schemaHints\", \"ShipMethod STRING\")\n .option(\"cloudFiles.schemaHints\", \"SubTotal FLOAT\")\n .option(\"cloudFiles.schemaHints\", \"TaxAmt FLOAT\")\n .option(\"cloudFiles.schemaHints\", \"Freight FLOAT\")\n .option(\"cloudFiles.schemaHints\", \"TotalDue FLOAT\")\n .option(\"cloudFiles.schemaHints\", \"SalesOrderDetailID INT\")\n .option(\"cloudFiles.schemaHints\", \"OrderQty SMALLINT\")\n .option(\"cloudFiles.schemaHints\", \"ProductID INT\")\n .option(\"cloudFiles.schemaHints\", \"UnitPrice FLOAT\")\n .option(\"cloudFiles.schemaHints\", \"UnitPriceDiscount FLOAT\")\n .option(\"cloudFiles.schemaHints\", \"LineTotal DECIMAL\")\n .option(\"cloudFiles.schemaHints\", \"rowguid STRING\")\n .option(\"cloudFiles.schemaHints\", \"ModifiedDate TIMESTAMP\")\n .option(\"cloudFiles.schemaLocation\", output_bronze)\n .option(\"cloudFiles.inferColumnTypes\", \"true\")\n .option(\"multiLine\", \"true\")\n .load(stream_dir)\n .createOrReplaceTempView(\"orders_raw_tempview\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2f88eda-452e-4368-bdc8-4a42edf0fc22","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n/* Add Metadata for Traceability */\nCREATE OR REPLACE TEMPORARY VIEW orders_bronze_tempview AS (\n  SELECT *, current_timestamp() receipt_time, input_file_name() source_file\n  FROM orders_raw_tempview\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3dc8433f-d57b-4121-856b-a9dbc5188bab","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM orders_bronze_tempview"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b46e12c7-4cb5-4e67-a0d8-838945356815","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(spark.table(\"orders_bronze_tempview\")\n      .writeStream\n      .format(\"delta\")\n      .option(\"checkpointLocation\", f\"{output_bronze}/_checkpoint\")\n      .outputMode(\"append\")\n      .table(\"fact_orders_bronze\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4599943a-c151-4aef-a835-d40880ed5911","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 6.2. Silver Table: Include Reference Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"638b3cf4-e996-4e0a-bbd6-cdf37e456c83","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.readStream\n  .table(\"fact_orders_bronze\")\n  .createOrReplaceTempView(\"orders_silver_tempview\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dcd901e1-fb41-4372-a2a7-d979ac10271b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM orders_silver_tempview"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d258589-613f-464f-bef2-f5119d9b2d1e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED orders_silver_tempview"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9faf92e2-80e8-4564-8485-168e743161d4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW fact_orders_silver_tempview AS (\n  SELECT t.SalesOrderID\n    , t.RevisionNumber\n    , od.MonthName AS OrderMonth\n    , od.WeekDayName AS OrderDayName\n    , od.Day AS OrderDay\n    , od.Year AS OrderYear\n    , dd.MonthName AS DueMonth\n    , dd.WeekDayName AS DueDayName\n    , dd.Day AS DueDate\n    , dd.Year AS DueYear\n    , sd.MonthName AS ShipMonth\n    , sd.WeekDayName AS ShipDayName\n    , sd.Day AS ShipDay\n    , sd.Year AS ShipYear\n    , t.Status\n    , t.OnlineOrderFlag\n    , t.SalesOrderNumber\n    , t.PurchaseOrderNumber\n    , t.AccountNumber\n    , c.CustomerID\n    , c.FirstName\n    , c.LastName\n    , t.ShipToAddressID\n    , sa.AddressLine1 AS ShipToAddressLine1\n    , sa.AddressLine2 AS ShipToAddressLine2\n    , sa.City AS ShipToCity\n    , sa.StateProvince AS ShipToStateProvince\n    , sa.PostalCode AS ShipToPostalCode\n    , t.BillToAddressID\n    , ba.AddressLine1 AS BillToAddressLine1\n    , ba.AddressLine2 AS BillToAddressLine2\n    , ba.City AS BillToCity\n    , ba.StateProvince AS BillToStateProvince\n    , ba.PostalCode AS BillToPostalCode\n    , t.ShipMethod\n    , t.SubTotal\n    , t.TaxAmt\n    , t.Freight\n    , t.TotalDue\n    , t.SalesOrderDetailID\n    , t.OrderQty\n    , p.ProductID\n    , p.ProductNumber\n    , t.UnitPrice\n    , t.UnitPriceDiscount\n    , t.LineTotal\n    , t.rowguid\n    , t.ModifiedDate\n    , t.receipt_time\n    , t.source_file\n  FROM orders_silver_tempview t\n  INNER JOIN adventure_works.dim_customer c\n  ON t.CustomerID = c.CustomerID\n  INNER JOIN adventure_works.dim_address sa\n  ON t.ShipToAddressID = CAST(sa.AddressID AS BIGINT)\n  INNER JOIN adventure_works.dim_address ba\n  ON t.BillToAddressID = CAST(ba.AddressID AS BIGINT)\n  INNER JOIN adventure_works.dim_product p\n  ON t.ProductID = p.ProductID\n  INNER JOIN adventure_works.dim_date od\n  ON CAST(t.OrderDate AS DATE) = od.Date\n  INNER JOIN adventure_works.dim_date dd\n  ON CAST(t.DueDate AS DATE) = dd.Date\n  INNER JOIN adventure_works.dim_date sd\n  ON CAST(t.ShipDate AS DATE) = sd.Date)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80e01ecb-47a8-417d-a1c7-3e09aebe2b10","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(spark.table(\"fact_orders_silver_tempview\")\n      .writeStream\n      .format(\"delta\")\n      .option(\"checkpointLocation\", f\"{output_silver}/_checkpoint\")\n      .outputMode(\"append\")\n      .table(\"fact_orders_silver\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb03f3db-7468-411c-866e-a299aab804e6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM fact_orders_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d03943d-95d0-4e9c-a692-8c67457f0215","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED adventure_works.fact_orders_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ccd554f3-868a-45fb-948a-52ac9ecfbfb8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 6.4. Gold Table: Perform Aggregations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ff89174-94f8-4e8d-9d0d-0e8004a2aae3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT CustomerID\n  , LastName\n  , FirstName\n  , OrderMonth\n  , COUNT(ProductID) AS ProductCount\nFROM adventure_works.fact_orders_silver\nGROUP BY CustomerID, LastName, FirstName, OrderMonth\nORDER BY ProductCount DESC"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8af068e8-f46e-4955-8edd-b21e844c7f79","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT pc.CustomerID\n  , os.LastName AS CustomerName\n  , os.ProductNumber\n  , pc.ProductCount\nFROM adventure_works.fact_orders_silver AS os\nINNER JOIN (\n  SELECT CustomerID\n  , COUNT(ProductID) AS ProductCount\n  FROM adventure_works.fact_orders_silver\n  GROUP BY CustomerID\n) AS pc\nON pc.CustomerID = os.CustomerID\nORDER BY ProductCount DESC"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2960743-bdce-45e7-9296-8d3e118d1891","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Final Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":479912801394479}},"nbformat":4,"nbformat_minor":0}
